Proximal Policy Optimization (PPO)

ÄÃ¢y lÃ  kho mÃ£ triá»ƒn khai thuáº­t toÃ¡n Proximal Policy Optimization (PPO) â€” má»™t trong nhá»¯ng phÆ°Æ¡ng phÃ¡p há»c tÄƒng cÆ°á»ng phá»• biáº¿n vÃ  hiá»‡u quáº£, Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi OpenAI.

ğŸ” Giá»›i thiá»‡u

PPO lÃ  má»™t thuáº­t toÃ¡n há»c chÃ­nh sÃ¡ch dá»±a trÃªn chÃ­nh sÃ¡ch gradient (policy gradient), má»¥c tiÃªu lÃ  tá»‘i Æ°u chÃ­nh sÃ¡ch má»™t cÃ¡ch á»•n Ä‘á»‹nh vÃ  hiá»‡u quáº£, trÃ¡nh cÃ¡c cáº­p nháº­t quÃ¡ lá»›n cÃ³ thá»ƒ lÃ m phÃ¢n tÃ¡n chÃ­nh sÃ¡ch.

âš™ï¸ CÃ¡ch hoáº¡t Ä‘á»™ng

Thu tháº­p dá»¯ liá»‡u: Cháº¡y chÃ­nh sÃ¡ch hiá»‡n táº¡i trong mÃ´i trÆ°á»ng vÃ  ghi láº¡i cÃ¡c tráº£i nghiá»‡m (tráº¡ng thÃ¡i, hÃ nh Ä‘á»™ng, pháº§n thÆ°á»Ÿng).

Æ¯á»›c tÃ­nh lá»£i Ã­ch (Advantage): TÃ­nh giÃ¡ trá»‹ lá»£i Ã­ch Ì‚ cho má»—i hÃ nh Ä‘á»™ng, thÆ°á»ng dÃ¹ng GAE (Generalized Advantage Estimation).

Cáº­p nháº­t chÃ­nh sÃ¡ch: Tá»‘i Æ°u hÃ m máº¥t mÃ¡t PPO vá»›i cÆ¡ cháº¿ clipping Ä‘á»ƒ giá»›i háº¡n má»©c thay Ä‘á»•i cá»§a chÃ­nh sÃ¡ch.

ğŸ“ CÃ´ng thá»©c hÃ m máº¥t mÃ¡t

L^{\text{clip}}(\theta) = \mathbb{E}_t \big[ \min \big( r_t(\theta) \hat{A}_t,\; \mathrm{clip}(r_t(\theta),1-\epsilon,1+\epsilon) \hat{A}_t \big) \big]

: Tá»· lá»‡ xÃ¡c suáº¥t thá»±c hiá»‡n hÃ nh Ä‘á»™ng theo chÃ­nh sÃ¡ch má»›i so vá»›i chÃ­nh sÃ¡ch cÅ©.

: Æ¯á»›c tÃ­nh lá»£i Ã­ch, thá»ƒ hiá»‡n Ä‘á»™ tá»‘t hÆ¡n cá»§a hÃ nh Ä‘á»™ng so vá»›i má»©c trung bÃ¬nh.

: Tham sá»‘ clip, thÆ°á»ng Ä‘áº·t khoáº£ng 0.1â€“0.2 Ä‘á»ƒ giá»›i háº¡n  trong [1âˆ’Îµ, 1+Îµ].

HÃ m clip giÃºp ngÄƒn khÃ´ng cho policy update vÆ°á»£t quÃ¡ vÃ¹ng an toÃ n, giá»¯ á»•n Ä‘á»‹nh quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

ğŸ›  CÃ i Ä‘áº·t

# Táº¡o mÃ´i trÆ°á»ng áº£o vÃ  cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
git clone <Ä‘Æ°á»ng dáº«n repo>
cd <tÃªn thÆ° má»¥c>
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
venv\\Scripts\\activate  # Windows
pip install -r requirements.txt

requirements.txt cá»§a báº¡n cÃ³ thá»ƒ bao gá»“m:

torch
gym
numpy

ğŸš€ Sá»­ dá»¥ng

Cháº¡y huáº¥n luyá»‡n PPO trÃªn mÃ´i trÆ°á»ng CartPole-v1:

python train_ppo.py --env CartPole-v1

Tuá»³ chá»‰nh tham sá»‘ hyperparameter trong file cáº¥u hÃ¬nh hoáº·c dÃ²ng lá»‡nh.

ğŸ“š Tham kháº£o

Schulman et al., Proximal Policy Optimization Algorithms (2017): https://arxiv.org/abs/1707.06347

OpenAI Spinning Up PPO Tutorial: https://spinningup.openai.com/en/latest/algorithms/ppo.html

