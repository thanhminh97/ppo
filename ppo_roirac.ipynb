{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "execution": {
          "iopub.execute_input": "2025-05-10T11:26:51.312529Z",
          "iopub.status.busy": "2025-05-10T11:26:51.312152Z",
          "iopub.status.idle": "2025-05-10T11:26:51.316909Z",
          "shell.execute_reply": "2025-05-10T11:26:51.316049Z",
          "shell.execute_reply.started": "2025-05-10T11:26:51.312506Z"
        },
        "id": "azsntCYCBWvp",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#!pip install \"gym [accept-rom-license, atari]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-10T11:26:51.318621Z",
          "iopub.status.busy": "2025-05-10T11:26:51.318294Z",
          "iopub.status.idle": "2025-05-10T11:26:58.665140Z",
          "shell.execute_reply": "2025-05-10T11:26:58.664042Z",
          "shell.execute_reply.started": "2025-05-10T11:26:51.318594Z"
        },
        "id": "qdp31bz7BWvq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install gym[atari,accept-rom-license] --quiet\n",
        "!pip install ale-py --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xia_yjSue3eb",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srnqMbBge3eb",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T00:52:35.939983Z",
          "iopub.status.busy": "2025-05-12T00:52:35.939527Z",
          "iopub.status.idle": "2025-05-12T00:52:35.948923Z",
          "shell.execute_reply": "2025-05-12T00:52:35.947463Z",
          "shell.execute_reply.started": "2025-05-12T00:52:35.939952Z"
        },
        "id": "Yx6Rqamwe3ec",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "def compute_gae(next_value, rewards, masks, values, gamma=0.999, tau=0.95):\n",
        "    # Similar to calculating the returns we can start at the end of the sequence and go backwards\n",
        "    gae = 0\n",
        "    returns = deque()\n",
        "    gae_logger = deque()\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        # Calculate the current delta value\n",
        "        delta = rewards[step] + gamma * next_value * masks[step] - values[step]\n",
        "\n",
        "        # The GAE is the decaying sum of these delta values\n",
        "        gae = delta + gamma * tau * masks[step] * gae\n",
        "        # Get the new next value\n",
        "        next_value = values[step]\n",
        "\n",
        "\n",
        "\n",
        "        # If we add the value back to the GAE we get a TD approximation for the returns\n",
        "        # which we can use to train the Value function\n",
        "        returns.appendleft(gae + values[step])\n",
        "        gae_logger.appendleft(gae)\n",
        "\n",
        "\n",
        "    return returns, gae_logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T00:52:36.290834Z",
          "iopub.status.busy": "2025-05-12T00:52:36.290316Z",
          "iopub.status.idle": "2025-05-12T00:52:38.699632Z",
          "shell.execute_reply": "2025-05-12T00:52:38.698225Z",
          "shell.execute_reply.started": "2025-05-12T00:52:36.290791Z"
        },
        "id": "z_KzPtYXBWvq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch as T\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, envs):\n",
        "        super(Agent, self).__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(np.array(envs.observation_space.shape).prod(), 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 1), std=1.0),\n",
        "        )\n",
        "        self.actor = nn.Sequential(\n",
        "            layer_init(nn.Linear(np.array(envs.observation_space.shape).prod(), 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, envs.action_space.n), std=0.01),\n",
        "        )\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        logits = self.actor(x)\n",
        "        probs = Categorical(logits=logits)\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T00:52:38.702632Z",
          "iopub.status.busy": "2025-05-12T00:52:38.701984Z",
          "iopub.status.idle": "2025-05-12T00:52:38.710199Z",
          "shell.execute_reply": "2025-05-12T00:52:38.708843Z",
          "shell.execute_reply.started": "2025-05-12T00:52:38.702586Z"
        },
        "id": "wUguwQoqBWvr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def ppo_loss(new_dist, old_log_probs, advantages, clip_param):\n",
        "      new_log_probs = new_dist\n",
        "      ratio = torch.exp(new_log_probs - old_log_probs)\n",
        "\n",
        "      surr1 = - advantages * ratio\n",
        "\n",
        "      surr2 = - advantages * torch.clamp(ratio, 1- clip_param, 1 + clip_param)\n",
        "\n",
        "      actor_loss = torch.max(surr1, surr2)\n",
        "\n",
        "\n",
        "      return actor_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T00:52:38.712108Z",
          "iopub.status.busy": "2025-05-12T00:52:38.711693Z",
          "iopub.status.idle": "2025-05-12T00:52:38.727826Z",
          "shell.execute_reply": "2025-05-12T00:52:38.726655Z",
          "shell.execute_reply.started": "2025-05-12T00:52:38.712050Z"
        },
        "id": "WtkGB1uWBWvr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def clipped_critic_loss(new_value, old_value, returns, clip_param):\n",
        "      vf_loss1 = (new_value - returns)**2\n",
        "\n",
        "    # 2. MSE/L2 loss on the clipped value and the returns\n",
        "    # Here we create an \"approximation\" of the new value (aka the current value) by finding the difference\n",
        "    # between the \"new\" and \"old\" value and adding a clipped amount back to the old value\n",
        "      vpredclipped = old_value + torch.clamp(new_value - old_value, -clip_param, clip_param)\n",
        "    # Note that we ONLY backprop through the new value\n",
        "      vf_loss2 = (vpredclipped - returns)**2\n",
        "\n",
        "    # 3. Take the MAX between the two losses\n",
        "    # This trick has the effect of only updating the current value DIRECTLY if is it WORSE (higher error)\n",
        "    # than the old value.\n",
        "    # If the old value was worse then the \"approximation\" will be worse and we update\n",
        "    # the new value only a little bit!\n",
        "      critic_loss = torch.max(vf_loss1, vf_loss2)\n",
        "\n",
        "    # 4. Return the Expectation over the batch\n",
        "      return 0.5 * critic_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "D5uIcyqie3ed",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T01:00:53.453508Z",
          "iopub.status.busy": "2025-05-12T01:00:53.453030Z",
          "iopub.status.idle": "2025-05-12T01:00:53.472342Z",
          "shell.execute_reply": "2025-05-12T01:00:53.470472Z",
          "shell.execute_reply.started": "2025-05-12T01:00:53.453465Z"
        },
        "id": "iX898-yLe3ed",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def xuly_dulieu(env, model, gamma, tau, device,tong_up):\n",
        "    obs= env.reset()\n",
        "    dulieu = {\n",
        "        \"obs\": [],\n",
        "        \"action\": [],\n",
        "        \"reward\": [],\n",
        "        \"logprob\": [],\n",
        "        \"value\": [],\n",
        "        \"done\": []\n",
        "    }\n",
        "\n",
        "    for _ in range(tong_up):\n",
        "        with torch.no_grad():\n",
        "            obs_array = np.array(obs)\n",
        "            obs_tensor = torch.tensor(obs_array, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            action, logprob, _, value = model.get_action_and_value(obs_tensor)\n",
        "        next_obs, reward, terminated, _ = env.step(action[0].cpu().numpy())\n",
        "        done = 0 if terminated else 1\n",
        "\n",
        "        dulieu[\"obs\"].append(obs_array)  # Save as np.array to avoid LazyFrames\n",
        "        dulieu[\"action\"].append(action)\n",
        "        dulieu[\"reward\"].append(reward)\n",
        "        dulieu[\"logprob\"].append(logprob)\n",
        "        dulieu[\"value\"].append(value)\n",
        "        dulieu[\"done\"].append(done)\n",
        "\n",
        "        obs = next_obs\n",
        "        if terminated :\n",
        "            obs = env.reset()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        next_value = model.get_value(\n",
        "            torch.tensor(np.array(obs), dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        )\n",
        "        next_value=next_value.view(-1)\n",
        "\n",
        "    # GAE\n",
        "    with torch.no_grad():\n",
        "        returns, advantages = compute_gae(\n",
        "        next_value,\n",
        "        dulieu[\"reward\"],\n",
        "        dulieu[\"done\"],\n",
        "        dulieu[\"value\"],\n",
        "        gamma,\n",
        "        tau\n",
        "    )\n",
        "\n",
        "    advantages = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
        "    # Convert everything to tensors and shuffle\n",
        "    obs_tensor = torch.tensor(np.array(dulieu[\"obs\"]), dtype=torch.float32).to(device)\n",
        "    action_tensor = torch.cat(dulieu[\"action\"]).to(device)\n",
        "    logprob_tensor = torch.cat(dulieu[\"logprob\"]).to(device)\n",
        "    value_tensor = torch.cat(dulieu[\"value\"]).squeeze(-1).to(device)\n",
        "    return_tensor = torch.tensor(returns, dtype=torch.float32).to(device)\n",
        "\n",
        "    b_states = obs_tensor.reshape((-1,) + env.observation_space.shape)\n",
        "    b_actions = action_tensor.reshape((-1,) +  env.action_space.shape)\n",
        "    b_logprobs= logprob_tensor.reshape(-1)\n",
        "    b_advantages = advantages.reshape(-1)\n",
        "    b_returns = return_tensor.reshape(-1)\n",
        "    b_values = value_tensor.reshape(-1)\n",
        "\n",
        "    rs = torch.tensor(dulieu[\"reward\"]).sum()\n",
        "\n",
        "    minibatch = {\n",
        "        \"obs\": b_states,\n",
        "        \"action\":  b_actions,\n",
        "        \"logprob\": b_logprobs,\n",
        "        \"value\":  b_values,\n",
        "        \"returns\": b_returns,\n",
        "        \"advantage\": b_advantages,\n",
        "    }\n",
        "\n",
        "    return minibatch, rs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "SGQev1Xxe3ed",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T01:18:25.381960Z",
          "iopub.status.busy": "2025-05-12T01:18:25.381596Z",
          "iopub.status.idle": "2025-05-12T01:18:25.393599Z",
          "shell.execute_reply": "2025-05-12T01:18:25.392266Z",
          "shell.execute_reply.started": "2025-05-12T01:18:25.381934Z"
        },
        "id": "teeyFIv_e3ed",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def ppo_update(data_buffer, ppo_epochs, clip_param, model, optimizer, device, minibatch_size=32):\n",
        "\n",
        "    obs =data_buffer[\"obs\"].to(device)\n",
        "    logprob =data_buffer[\"logprob\"].to(device)\n",
        "    values = data_buffer[\"value\"].to(device)\n",
        "    returns = data_buffer[\"returns\"].to(device)\n",
        "    actions = data_buffer[\"action\"]\n",
        "    advantages = data_buffer[\"advantage\"].to(device)\n",
        "    batch_size = obs.shape[0]\n",
        "    for _ in range(ppo_epochs):\n",
        "        idx = torch.randperm(batch_size)\n",
        "        for start in range(0, batch_size, minibatch_size):\n",
        "            end = start + minibatch_size\n",
        "            mb_idx = idx[start:end]\n",
        "            mb_obs = obs[mb_idx]\n",
        "\n",
        "            mb_old_logprob = logprob[mb_idx]\n",
        "            mb_advantage =advantages[mb_idx]\n",
        "            mb_advantages = (mb_advantage - mb_advantage.mean()) / (mb_advantage.std() +1e-8)\n",
        "            mb_returns = returns[mb_idx]\n",
        "            mb_old_values = values[mb_idx]\n",
        "\n",
        "                # 4. Tính toán giá trị mới\n",
        "            _, new_logprob, entropy, new_value = model.get_action_and_value(mb_obs,actions.long()[mb_idx])\n",
        "\n",
        "                # 5. Tính loss\n",
        "            actor_loss = ppo_loss(new_logprob, mb_old_logprob, mb_advantages, clip_param)\n",
        "            critic_loss = clipped_critic_loss(new_value.view(-1), mb_old_values, mb_returns, clip_param)\n",
        "            loss_entropy = entropy.mean()\n",
        "\n",
        "            total_loss = actor_loss + 0.5 * critic_loss + 0.01* -loss_entropy\n",
        "\n",
        "                # 6. Tối ưu hóa\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "\n",
        "    return loss_entropy  # hoặc return None nếu không cần\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-12T01:18:25.604834Z",
          "iopub.status.busy": "2025-05-12T01:18:25.604417Z",
          "iopub.status.idle": "2025-05-12T01:18:25.622419Z",
          "shell.execute_reply": "2025-05-12T01:18:25.620941Z",
          "shell.execute_reply.started": "2025-05-12T01:18:25.604793Z"
        },
        "id": "zB05HwdNe3ee",
        "outputId": "f440cc48-93b7-4a77-e9d9-c15e9a763b57",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "lr = 2e-4\n",
        "ppo_epochs = 4\n",
        "clip_param = 0.2\n",
        "gamma=0.99\n",
        "tau=0.95\n",
        "seed=42\n",
        "\n",
        "name=\"LunarLander-v2\"\n",
        "env = gym.make(name)\n",
        "env = gym.wrappers.NormalizeReward(env)\n",
        "env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "agent=Agent(env).to(device)\n",
        "optimizer = optim.Adam(agent.parameters(), lr=lr, eps=1e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2025-05-12T01:18:26.018659Z",
          "iopub.status.busy": "2025-05-12T01:18:26.018276Z",
          "iopub.status.idle": "2025-05-12T03:03:21.932160Z",
          "shell.execute_reply": "2025-05-12T03:03:21.930428Z",
          "shell.execute_reply.started": "2025-05-12T01:18:26.018632Z"
        },
        "id": "hCb--ct8e3ee",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "fe50528d-6c86-4612-857d-80c16ed18acf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "tong_up=2048\n",
        "bat_dau=1\n",
        "total_timesteps=10000000\n",
        "num_updates = total_timesteps // tong_up\n",
        "\n",
        "for update in range(bat_dau, num_updates + 1):\n",
        "\n",
        "   fraction = 1.0 - ((update - 1.0) / num_updates)\n",
        "   lr_current = fraction * lr\n",
        "   optimizer.param_groups[0]['lr'] = lr_current\n",
        "\n",
        "   du_lieu,rs= xuly_dulieu(env,agent,gamma, tau,device,tong_up)\n",
        "\n",
        "   ep=ppo_update(du_lieu, ppo_epochs, clip_param,agent,optimizer,device,minibatch_size=128)\n",
        "   if update % 50 ==0:\n",
        "       torch.save({\n",
        "            'update': update,\n",
        "            'model_state_dict': agent.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict()\n",
        "        }, f\"agent_update_{update}.pth\")\n",
        "   if update % 20 ==0:\n",
        "\n",
        "       print(f\"num_up: {update}/{num_updates} -- total_timesteps: {update*tong_up}/{total_timesteps}\")\n",
        "       print(f\"phần thưởng: {rs.item():.3f} -- entropy: {ep.item():.3f}\")\n",
        "       k=tes()\n",
        "       print( \"phần thưởng thử\",k.item())\n",
        "       print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1ws-WyKe3ee",
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T03:03:27.550885Z",
          "iopub.status.busy": "2025-05-12T03:03:27.550481Z",
          "iopub.status.idle": "2025-05-12T03:03:28.462717Z",
          "shell.execute_reply": "2025-05-12T03:03:28.461534Z",
          "shell.execute_reply.started": "2025-05-12T03:03:27.550848Z"
        },
        "id": "VrlGqInV1t74",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Dict, Optional, Iterable, Callable\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib import animation\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.error import DependencyNotInstalled\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T03:03:28.887653Z",
          "iopub.status.busy": "2025-05-12T03:03:28.887047Z",
          "iopub.status.idle": "2025-05-12T03:03:28.895721Z",
          "shell.execute_reply": "2025-05-12T03:03:28.894431Z",
          "shell.execute_reply.started": "2025-05-12T03:03:28.887619Z"
        },
        "id": "ziVuphul1vRO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def display_video(frames):\n",
        "    # Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb\n",
        "    orig_backend = matplotlib.get_backend()\n",
        "    matplotlib.use('Agg')\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "    matplotlib.use(orig_backend)\n",
        "    ax.set_axis_off()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_position([0, 0, 1, 1])\n",
        "    im = ax.imshow(frames[0])\n",
        "    def update(frame):\n",
        "        im.set_data(frame)\n",
        "        return [im]\n",
        "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                    interval=50, blit=True, repeat=False)\n",
        "    return HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T03:03:54.877128Z",
          "iopub.status.busy": "2025-05-12T03:03:54.876700Z",
          "iopub.status.idle": "2025-05-12T03:03:54.886595Z",
          "shell.execute_reply": "2025-05-12T03:03:54.885170Z",
          "shell.execute_reply.started": "2025-05-12T03:03:54.877095Z"
        },
        "id": "0r7_aaqfBWvs",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v2\",render_mode='rgb_array')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T03:04:43.694251Z",
          "iopub.status.busy": "2025-05-12T03:04:43.693835Z",
          "iopub.status.idle": "2025-05-12T03:04:44.581062Z",
          "shell.execute_reply": "2025-05-12T03:04:44.579863Z",
          "shell.execute_reply.started": "2025-05-12T03:04:43.694220Z"
        },
        "id": "oSKP8fQue3ef",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "obs= env.reset()\n",
        "k=0\n",
        "frames = []\n",
        "for i in range(1024):\n",
        "    with torch.no_grad():\n",
        "        obs_array = np.array(obs)\n",
        "        obs_tensor = torch.tensor(obs_array, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "        action, logprob, _, value = agent.get_action_and_value(obs_tensor)\n",
        "        next_obs, reward, done, info = env.step(action[0].cpu().numpy())\n",
        "        obs=next_obs\n",
        "        k += reward\n",
        "        img = env.render()\n",
        "        frames.append(img[0])\n",
        "        if done:\n",
        "            break\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-12T03:04:51.875185Z",
          "iopub.status.busy": "2025-05-12T03:04:51.874766Z",
          "iopub.status.idle": "2025-05-12T03:04:55.258572Z",
          "shell.execute_reply": "2025-05-12T03:04:55.257387Z",
          "shell.execute_reply.started": "2025-05-12T03:04:51.875152Z"
        },
        "id": "Fr3uGAcc2gIn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "display_video(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DDIHCdck66b",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30732,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
